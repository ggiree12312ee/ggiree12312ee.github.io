<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/t-rex-v4.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Continual Personalization for Diffusion Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/ggiree12312ee" target="_blank">Yu-Chien Liao</a><sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">
                  <a href="https://github.com/TousakaNagio" target="_blank">Jr-Jen Chen</a><sup style="color:#6fbf73;">1</sup>,</span>
                  <span class="author-block">
                    <a href="https://github.com/hsi-che-lin" target="_blank">Chi-Pin Huang</a><sup style="color:#6fbf73;">1</sup>,</span>
                  </span>
                  </div>
                  
              <!-- New row of authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://chuyu.org/?fbclid=IwZXh0bgNhZW0CMTAAAR10wQyaYpKvgATLl8pavKrLJyoATjrC6KypMKehka00fbgXDyuIug9BkfA_aem_w8m5sW20POmHM-tjJZHZSg" target="_blank">Yu-Chu Yu</a><sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.microsoft.com/en-us/research/people/yenche/" target="_blank">Ci-Siang Lin</a><sup style="color:#ffac33;">2</sup>,</span>
                <span class="author-block">
                  <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Meng-Lin Wu</a><sup style="color:#6fbf73;">2</sup>,</span>
                <span class="author-block">
                  <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a><sup style="color:#6fbf73;">1</sup>,</span>
                </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup style="color:#6fbf73;">1</sup>National Taiwan University,</span>
                    <span class="author-block"><sup style="color:#ffac33;">2</sup>QualComm</span><br>
                    <span class="author-block">ICCV 2025 Poster</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- PDF Link. -->
                      <span class="link-block">
                        <!-- @PAN TODO: change links -->
                        <a href="https://arxiv.org/abs/2510.02296" 
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>arXiv</span>
                        </a>
                      </span>
                      <span class="link-block">
                        <a href=""
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-github"></i>
                          </span>
                          <span>Code Coming Soon</span>
                          </a>
                      </span>
                    </div>
                  </div>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
        <div class="content has-text-centered">
          <img src="static/images/cns_teaser.jpg" alt="geometric reasoning" width="100%"/>
          <p>We present <b>C</b>oncept <b>N</b>euron <b>S</b>election, CNS, a simple yet effective approach to incrementally customize visual concepts. By finetuning concept-related neurons, CNS preserves the zero-shot capabilities of pretrained diffusion models and alleviates catastrophic forgetting problems.</p>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üîîNews</h2>
        <div class="content has-text-justified">
          <p>
            <b> [2024-06-06]: Submit to NeurIPS 2024 Datasets and Benchmarks Track</b>
          </p>
      </div>       -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection, a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization. </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>Latent Diffusion Models (LDMs) represent a milestone for image generation task by leveraging vast collection of text-image pairs and denoising process. LDMs enable users to create high-quality images through simple text prompts. Yet, LDMs often fall short in generating user-specific concepts e.g. their pets, a scene in a national park, which poses a practical challenge in text-to-image generation since these user-specific concepts are hard to describe directly by text. To address this issue, personalization techniques allow users to adapt LDMs to generate their desired specific content by finetuning it with their own examples. Existing methods tend to make the assumption that personalized concepts are fixed, which means that storing all of the personalized model weights and multiple times of fusion are required if users need different numbers of personalized concepts across different images. However, in the realistic application scenario, users' personalized concepts never remain static and will incrementally increase. A practical scenario is that users can continuously assign new concepts to a single diffusion model and additional computation effort while generating new composite images is not required. Furthermore, learning the concepts separately usually results in conflict while fusing concepts together.</p>
          <p>In this paper, we propose CNS, <b>C</b>oncept <b>N</b>euron <b>S</b>election, which is able to identify the neurons relevant to personalized target concepts in an incremental fashion. Specifically, CNS allows one to automatically identify the neurons related to (few-shot) images of a concept (i.e., <i>base neurons</i>) and those related to the general image synthesis (i.e., <i>general neurons</i>) using diffusion models. By excluding general neurons from the base ones, the <i>concept neurons</i> describing the input concept of interest can be selected. Moreover, in order to achieve continual learning, an incremental finetuning scheme with such concept neurons is also proposed. Different from existing continual learning methods, we do not require to train and store any extra LoRAs when handling multiple concepts. As a result, our CNS framework is able to achieve effective continual concept personalization, not only preventing the catastrophic forgetting but also preserving the zero-shot capability of pretrained text-to-image diffusion models.</p>
        </div>
        <!-- <div class="content has-text-centered">
          <img src="static/images/venn_v8.jpg" alt="algebraic reasoning" class="center">
        </div> -->
    </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
          <img src="static/images/cns_architecture.jpg" alt="algebraic reasoning" class="center">
          <p> (a) Identify neurons highly responsive to the target concept (base neurons).
            (b) Search for the neurons that consistently activate across many unrelated prompts with diverse calibration set (general neurons).
            (c) Remove general neurons from the base neurons.
            (d) Only fine-tune concept neurons and special token standing for the concept.
             </p>
        </div>
          
        </div>
    </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitatively Result</h2>
        <div class="content has-text-justified">
        <div class="content has-text-centered">
          <img src="static/images/main_v8.jpg" alt="algebraic reasoning" class="center">
          <p> In stage I, we collect event pairs from two video sources.
            In stage II, we score and categorize the event pairs into four relation types.
            In stage III, the (M)LLM generates question-answer pairs by our carefully written few-shot demonstrations.
            In stage IV, the LLM self-evaluate the generated samples to reduce the human verification cost. </p>
        </div>
          
          
        </div>
    </div>
    </div>
<!-- 
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            To further distinguish the difference between <i>dataset</i> and other existing ones, we elaborate the benchmark details in Figure. 
            From the <i>breadth</i> perspective, the prior benchmarks are heavily focused on daily knowledge and common sense. 
            The covered image format is also limited. Our benchmark aims to cover college-level knowledge with 30 image formats including diagrams, 
            tables, charts, chemical structures, photos, paintings, geometric shapes, music sheets, medical images, etc. 
            In the <i>depth</i> aspect, the previous benchmarks normally require commonsense knowledge or simple physical or temporal reasoning. 
            In contrast, our benchmark requires deliberate reasoning with college-level subject knowledge.
        </p>
        <div class="content has-text-centered">
          <img src="static/images/main_v8.jpg" alt="algebraic reasoning" class="center">
          <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
        </div>
          
          
        </div>
    </div>
    </div> -->

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            We compare ReXTime to the two datasets, NExT-GQA and Ego4D-NLQ on the number of reasoning across time samples, certificate length, and QA-mIoU. The average certificate length in our dataset is considerably longer than in existing tasks. This suggests that effectively addressing our task requires models to have more advanced temporal reasoning abilities. The lower QA-mIoU in ReXTime indicates that an AI model needs to first locate the question event and then scan the rest of the visual events in the video to reason about the correct answer. This is more challenging because the reasoning and moment localization cannot be easily decomposed. For existing tasks, a model mostly needs to localize the question event and then reason within roughly the same span due to the higher QA-IoU.
        </p>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/dataset_comp1.png" alt="algebraic reasoning" width="75%"/>
              <p> We compare ReXTime with related datasets on temporal reasoning or moment localization, highlighting our uniqueness. ReXTime covers features from all similar video QA tasks. Notably, ‚Äúreasoning-across-time‚Äù emphasizes the cause and effect understanding between visual events.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/dataset_comp2.png" alt="arithmetic reasoning" width="75%"/>
              <p>Our comparison focuses on datasets with both question queries and moment localization features. We present a comprehensive report detailing the number of temporal reasoning samples on each split, certificate length (C.L.) and Question-Answer mean Intersection over Union (QA-mIoU) respectively. (<span>&#8224;</span>: Only counts temporal reasoning QA pairs. See supplementary in paper for details.)</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mmmu">Experiment Results</h1>
  </div>
</section>
<section class="section">
  <div class="container">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
        <div class="content">
          <div class="content has-text-justified">
            <p>
              We evaluate various models including open-source and proprietary models on the ReXTime benchmark. We consider two types of tasks: moment localization and video question answering (VQA).
              Our evaluation is conducted under a zero-shot setting and a fine-tuning setting if available.
            </p>
          </div>

          <div class="content has-text-centered">
            <button id="toggleButton" class="button is-rounded is-dark" onclick="changeButtonText()">"Proprietary models Leaderboard (Mini test set)</button>
          </div>

          <!-- Private Set Table -->
          <div id="table1" class="content">
            <p class="private-desc">This table shows the results of human and the proprietary models on 300 mini test set.</p>
            <table class="table is-striped is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th rowspan="2">Models</th>
                  <th colspan="3">Moment Localization</th>
                  <th colspan="2">VQA</th>
                </tr>
                <tr>
                  <th>mIoU</th>
                  <th>R @1 (IoU=0.3)</th>
                  <th>R @1 (IoU=0.5)</th>
                  <th>Accuracy(%)</th>
                  <th>Accuracy(%) @IoU ‚â• 0.5</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Human</td>
                  <td>61.11</td>
                  <td>74.30</td>
                  <td>62.85</td>
                  <td>87.98</td>
                  <td>58.51</td>
                </tr>
                <tr>
                  <td>GPT-4o</td>
                  <td>36.28</td>
                  <td>45.33</td>
                  <td>34.00</td>
                  <td>73.67</td>
                  <td>28.67</td>
                </tr>
                <tr>
                  <td>Claude3-Opus</td>
                  <td>23.61</td>
                  <td>30.67</td>
                  <td>17.67</td>
                  <td>68.67</td>
                  <td>13.67</td>
                </tr>
                <tr>
                  <td>Gemini-1.5-Pro</td>
                  <td>28.43</td>
                  <td>35.67</td>
                  <td>25.00</td>
                  <td>68.00</td>
                  <td>18.33</td>
                </tr>
                <tr>
                  <td>GPT-4V</td>
                  <td>26.74</td>
                  <td>33.33</td>
                  <td>22.00</td>
                  <td>63.33</td>
                  <td>16.67</td>
                </tr>
                <tr>
                  <td>Reka-Core</td>
                  <td>27.95</td>
                  <td>36.33</td>
                  <td>24.00</td>
                  <td>59.67</td>
                  <td>17.00</td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- Private Set Table -->
          <div id="table2" class="content hidden">
            <p class="open-mini-desc hidden">This table shows the results of open source models on 300 mini test set.</p>
            <table class="table is-striped is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th rowspan="2">Models</th>
                  <th colspan="3">Moment Localization</th>
                  <th colspan="2">VQA</th>
                </tr>
                <tr>
                  <th>mIoU</th>
                  <th>R @1 (IoU=0.3)</th>
                  <th>R @1 (IoU=0.5)</th>
                  <th>Accuracy(%)</th>
                  <th>Accuracy(%) @IoU ‚â• 0.5</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>UniVTG (zero-shot)</td>
                  <td>30.18</td>
                  <td>42.00</td>
                  <td>29.33</td>
                  <td>--</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>CG-DETR (zero-shot)</td>
                  <td>22.53</td>
                  <td>30.00</td>
                  <td>16.67</td>
                  <td>--</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>VTimeLLM (zero-shot)</td>
                  <td>19.37</td>
                  <td>27.67</td>
                  <td>16.00</td>
                  <td>37.33</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>TimeChat (zero-shot)</td>
                  <td>13.01</td>
                  <td>16.33</td>
                  <td>7.00</td>
                  <td>38.33</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>LITA (zero-shot)</td>
                  <td>24.76</td>
                  <td>34.33</td>
                  <td>20.00</td>
                  <td>35.00</td>
                  <td>--</td>
                </tr>

                <tr>
                  <td>UniVTG (fine-tuned)</td>
                  <td>34.82</td>
                  <td>53.00</td>
                  <td>35.33</td>
                  <td>--</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>CG-DETR (fine-tuned)</td>
                  <td>24.98</td>
                  <td>38.00</td>
                  <td>20.33</td>
                  <td>--</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>VTimeLLM (fine-tuned)</td>
                  <td>29.53</td>
                  <td>43.67</td>
                  <td>25.00</td>
                  <td>54.67</td>
                  <td>15.67</td>
                </tr>
                <tr>
                  <td>TimeChat (fine-tuned)</td>
                  <td>27.54</td>
                  <td>38.00</td>
                  <td>21.67</td>
                  <td>52.00</td>
                  <td>11.33</td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- Test Set Table -->
          <div id="table3" class="content hidden">
            <p class="open-zero-shot-desc hidden">This table shows the results of zero-shot performance of open source moment retrieval models and grounding MLLMs.</p>
            <table class="table is-striped is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th rowspan="2">Models</th>
                  <th colspan="3">Moment Localization</th>
                  <th>VQA</th>
                </tr>
                <tr>
                  <th>mIoU</th>
                  <th>R @1 (IoU=0.3)</th>
                  <th>R @1 (IoU=0.5)</th>
                  <th>Accuracy(%)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>UniVTG</td>
                  <td>28.07</td>
                  <td>41.45</td>
                  <td>26.85</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>CG-DETR</td>
                  <td>22.39</td>
                  <td>29.35</td>
                  <td>16.70</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>VTimeLLM</td>
                  <td>20.82</td>
                  <td>31.10</td>
                  <td>18.30</td>
                  <td>36.25</td>
                </tr>
                <tr>
                  <td>TimeChat</td>
                  <td>11.60</td>
                  <td>14.25</td>
                  <td>7.70</td>
                  <td>38.45</td>
                </tr>
                <tr>
                  <td>LITA</td>
                  <td>21.15</td>
                  <td>28.90</td>
                  <td>15.90</td>
                  <td>33.80</td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- Additional Table -->
          <div id="table4" class="content hidden">
            <p class="open-finetune-desc hidden">This table shows the results of fine-tuned performance of open source moment retrieval models and grounding MLLMs.</p>
            <table class="table is-striped is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th rowspan="2">Models</th>
                  <th colspan="3">Moment Localization</th>
                  <th colspan="2">VQA</th>
                </tr>
                <tr>
                  <th>mIoU</th>
                  <th>R @1 (IoU=0.3)</th>
                  <th>R @1 (IoU=0.5)</th>
                  <th>Accuracy(%)</th>
                  <th>Accuracy(%) @IoU ‚â• 0.5</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>UniVTG</td>
                  <td>34.73</td>
                  <td>53.55</td>
                  <td>34.70</td>
                  <td>--</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>CG-DETR</td>
                  <td>26.60</td>
                  <td>39.80</td>
                  <td>22.90</td>
                  <td>--</td>
                  <td>--</td>
                </tr>
                <tr>
                  <td>VTimeLLM</td>
                  <td>30.03</td>
                  <td>44.05</td>
                  <td>26.55</td>
                  <td>58.15</td>
                  <td>18.30</td>
                </tr>
                <tr>
                  <td>TimeChat</td>
                  <td>26.52</td>
                  <td>40.45</td>
                  <td>21.90</td>
                  <td>49.35</td>
                  <td>11.10</td>
                </tr>
                <!-- Add more rows as needed -->
              </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{chen2024rextime,
        title={ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos},
        author={Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Yu-Chiang Frank},
        journal={arXiv preprint arXiv:2406.19392},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page and <a href="https://mmmu-benchmark.github.io/" target="_blank">MMMU-Benchmark</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  function changeButtonText(currentIndex) {
    var button = document.getElementById('toggleButton');
    const labels = ["Proprietary Models Leaderboard (Mini test set)", "Open Source Models Leaderboard (Mini test set)", "Open Source Zero-shot Leaderboard", "Open Source Fine-tuned Leaderboard"];
    button.innerHTML = `<b>${labels[currentIndex]}</b> (Click to Switch)`;
  }

  function toggleTables() {
    var tables = [document.getElementById('table1'), document.getElementById('table2'), document.getElementById('table3'), document.getElementById('table4')];
    var descriptions = [document.querySelector('p.private-desc'), document.querySelector('p.open-mini-desc'), document.querySelector('p.open-zero-shot-desc'), document.querySelector('p.open-finetune-desc')];
    var currentIndex = tables.findIndex(table => !table.classList.contains('hidden'));
    tables[currentIndex].classList.add('hidden');
    descriptions[currentIndex].classList.add('hidden');
    currentIndex = (currentIndex + 1) % tables.length;
    tables[currentIndex].classList.remove('hidden');
    descriptions[currentIndex].classList.remove('hidden');
    changeButtonText(currentIndex);
  }

  document.getElementById('toggleButton').addEventListener('click', toggleTables);
</script>

<style>
  .hidden {
    display: none;
  }
</style>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
