<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Continual Personalization for Diffusion Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/t-cns-v1.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="white-space: nowrap; width: auto; max-width: 100%;">Continual Personalization for Diffusion Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/ggiree12312ee" target="_blank">Yu-Chien Liao*</a><sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">
                  <a href="https://github.com/TousakaNagio" target="_blank">Jr-Jen Chen*</a><sup style="color:#6fbf73;">1</sup>,</span>
                  <span class="author-block">
                    <a href="https://jasper0314-huang.github.io/" target="_blank">Chi-Pin Huang</a><sup style="color:#6fbf73;">1</sup>,</span>
                  </span>
                  </div>
                  
              <!-- New row of authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=ZkeWTScAAAAJ&hl=zh-TW" target="_blank">Ci-Siang Lin</a><sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=egUzoygAAAAJ&hl=en" target="_blank">Meng-Lin Wu</a><sup style="color:#ffac33;">2</sup>,</span>
                <span class="author-block">
                  <a href="https://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a><sup style="color:#6fbf73;">1</sup>,</span>
                </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span style="font-size: 2em; color:#c71d1d">ICCV 2025</span><br>
                    <span class="author-block"><sup style="color:#6fbf73;">1</sup>National Taiwan University,</span>
                    <span class="author-block"><sup style="color:#ffac33;">2</sup>QualComm</span><br>
                    <span><small>* stands for equal contribution</small></span><br>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- PDF Link. -->
                      <span class="link-block">
                        <!-- @PAN TODO: change links -->
                        <a href="https://arxiv.org/abs/2510.02296" 
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>arXiv</span>
                        </a>
                      </span>
                      <span class="link-block">
                        <a href=""
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-github"></i>
                          </span>
                          <span>Code Coming Soon</span>
                          </a>
                      </span>
                    </div>
                  </div>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
        <div class="content has-text-centered">
          <img src="static/images/cns_teaser.png" alt="geometric reasoning" width="100%"/>
          <p>
            <b>C</b>oncept <b>N</b>euron <b>S</b>election, CNS, an effective approach to incrementally customize visual concepts. By finetuning concept-related neurons, CNS preserves the zero-shot capabilities of pretrained diffusion models and alleviates catastrophic forgetting problems.</p>
        </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- Abstract. -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection, a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization. </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

            
<section class="section">
  <div class="container">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
          <img src="static/images/cns_architecture.png" alt="algebraic reasoning" class="center">
          <p> (a) Identify neurons highly responsive to the target concept (base neurons). <br>
            (b) Search for the neurons that consistently activate across many unrelated prompts with diverse calibration set (general neurons).<br>
            (c) Remove general neurons from the base neurons.<br>
            (d) Only fine-tune concept neurons and special token standing for the concept.
             </p>
        </div>
          
        </div>
    </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitatively Result</h2>
        <div class="content has-text-justified">
        <div class="content has-text-centered">
          <img src="static/images/cns_comparison_figure.png" alt="algebraic reasoning" class="center">
          <p>  Note that only Continual Diffusion and CNS are capable of performing continual personalization, while Mix-of-Show and Orthogonal Adaptation require to keep LoRAs for each concept for personalization. It can be seen that our personalized outputs match concepts learned across different time, alleviating appearance leakage and catastrophic forgetting problems. </p>
        </div>
        </div>
    </div>
    </div>

    <!-- <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Qualitatively Result</h2>
        <div class="content has-text-justified">
          <p>
            We compare ReXTime to the two datasets, NExT-GQA and Ego4D-NLQ on the number of reasoning across time samples, certificate length, and QA-mIoU. The average certificate length in our dataset is considerably longer than in existing tasks. This suggests that effectively addressing our task requires models to have more advanced temporal reasoning abilities. The lower QA-mIoU in ReXTime indicates that an AI model needs to first locate the question event and then scan the rest of the visual events in the video to reason about the correct answer. This is more challenging because the reasoning and moment localization cannot be easily decomposed. For existing tasks, a model mostly needs to localize the question event and then reason within roughly the same span due to the higher QA-IoU.
        </p>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/dataset_comp1.png" alt="algebraic reasoning" width="75%"/>
              <p> We compare ReXTime with related datasets on temporal reasoning or moment localization, highlighting our uniqueness. ReXTime covers features from all similar video QA tasks. Notably, “reasoning-across-time” emphasizes the cause and effect understanding between visual events.</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/dataset_comp2.png" alt="arithmetic reasoning" width="75%"/>
              <p>Our comparison focuses on datasets with both question queries and moment localization features. We present a comprehensive report detailing the number of temporal reasoning samples on each split, certificate length (C.L.) and Question-Answer mean Intersection over Union (QA-mIoU) respectively. (<span>&#8224;</span>: Only counts temporal reasoning QA pairs. See supplementary in paper for details.)</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div> -->

  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitatively Result</h2>
        <div class="content has-text-justified">
        <div class="content has-text-centered">
          <img src="static/images/cns_quantitative.PNG" alt="algebraic reasoning" class="center">
          <p>  
            In addition to the alignment-based metrics of CLIP-I and CLIP-T, we provide the computation estimates for different personalization methods. Note that memory requirements for GPU/CPU and computation time for multi-concept personalization indicate the <i>additional</i> costs for fusing concept weights previously learned. </p>
        </div>
        </div>
    </div>
    </div>

</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@InProceedings{Liao_2025_ICCV,
    author    = {Liao, Yu-Chien and Chen, Jr-Jen and Huang, Chi-Pin and Lin, Ci-Siang and Wu, Meng-Lin and Wang, Yu-Chiang Frank},
    title     = {Continual Personalization for Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2025},
    pages     = {15511-15520}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page and <a href="https://mmmu-benchmark.github.io/" target="_blank">MMMU-Benchmark</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  function changeButtonText(currentIndex) {
    var button = document.getElementById('toggleButton');
    const labels = ["Proprietary Models Leaderboard (Mini test set)", "Open Source Models Leaderboard (Mini test set)", "Open Source Zero-shot Leaderboard", "Open Source Fine-tuned Leaderboard"];
    button.innerHTML = `<b>${labels[currentIndex]}</b> (Click to Switch)`;
  }

  function toggleTables() {
    var tables = [document.getElementById('table1'), document.getElementById('table2'), document.getElementById('table3'), document.getElementById('table4')];
    var descriptions = [document.querySelector('p.private-desc'), document.querySelector('p.open-mini-desc'), document.querySelector('p.open-zero-shot-desc'), document.querySelector('p.open-finetune-desc')];
    var currentIndex = tables.findIndex(table => !table.classList.contains('hidden'));
    tables[currentIndex].classList.add('hidden');
    descriptions[currentIndex].classList.add('hidden');
    currentIndex = (currentIndex + 1) % tables.length;
    tables[currentIndex].classList.remove('hidden');
    descriptions[currentIndex].classList.remove('hidden');
    changeButtonText(currentIndex);
  }

  document.getElementById('toggleButton').addEventListener('click', toggleTables);
</script>

<style>
  .hidden {
    display: none;
  }
</style>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
